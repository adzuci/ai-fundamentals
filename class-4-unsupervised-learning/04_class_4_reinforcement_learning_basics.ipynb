{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Class 4 Notebook – Reinforcement Learning: Basics and Q-Learning Demo\n",
    "\n",
    "This notebook introduces **Reinforcement Learning (RL)**, a type of machine learning where an agent learns to make decisions by interacting with an environment.\n",
    "\n",
    "Unlike supervised learning (Classes 2–3) and unsupervised learning (other Class 4 notebooks), **reinforcement learning** involves:\n",
    "- An **agent** that takes actions\n",
    "- An **environment** that responds to actions\n",
    "- **Rewards** that guide learning\n",
    "- **Learning through trial and error**\n",
    "\n",
    "**Objective**: Understand reinforcement learning concepts and implement a simple Q-Learning example where an agent learns to navigate a grid world.\n",
    "\n",
    "**Model type**: Q-Learning (value-based reinforcement learning).\n",
    "\n",
    "**Key idea**: The agent learns a Q-table that stores the expected future rewards for each state-action pair. Through exploration and exploitation, the agent improves its policy over time.\n",
    "\n",
    "We'll follow a step-by-step workflow:\n",
    "\n",
    "1. Install/import libraries\n",
    "2. Understand RL concepts (agent, environment, rewards, Q-learning)\n",
    "3. Create a simple grid world environment\n",
    "4. Implement Q-Learning algorithm\n",
    "5. Train the agent\n",
    "6. Visualize the learned policy\n",
    "\n",
    "Run the first code cell to confirm your environment works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colab-badge",
   "metadata": {},
   "source": [
    "## Run in the browser (no local setup)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adzuci/ai-fundamentals/blob/class-4-unsupervised-learning/class-4-unsupervised-learning/04_class_4_reinforcement_learning_basics.ipynb)\n",
    "\n",
    "> Tip: This notebook assumes you're comfortable with basic Python, NumPy, and Matplotlib from Classes 2 and 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rl-context",
   "metadata": {},
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "**Supervised Learning** (Classes 2–3):\n",
    "- Learn from labeled examples (input → output pairs)\n",
    "- Goal: Predict the correct output for new inputs\n",
    "- Examples: House price prediction, image classification\n",
    "\n",
    "**Unsupervised Learning** (Class 4):\n",
    "- Learn from unlabeled data\n",
    "- Goal: Discover hidden patterns or structures\n",
    "- Examples: Clustering, dimensionality reduction\n",
    "\n",
    "**Reinforcement Learning** (This notebook):\n",
    "- Learn through interaction with an environment\n",
    "- Goal: Maximize cumulative reward over time\n",
    "- Examples: Game playing (Chess, Go), robotics, autonomous vehicles\n",
    "\n",
    "**Key Components**:\n",
    "- **Agent**: The learner/decision maker\n",
    "- **Environment**: The world the agent interacts with\n",
    "- **State**: Current situation\n",
    "- **Action**: What the agent can do\n",
    "- **Reward**: Feedback signal (positive = good, negative = bad)\n",
    "- **Policy**: Strategy for choosing actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1",
   "metadata": {},
   "source": [
    "## STEP 1: Install Required Libraries\n",
    "\n",
    "If running locally, install the required packages. In Colab, these are already available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (run this if needed)\n",
    "# Uncomment the line below if running locally and packages aren't installed\n",
    "# !pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2",
   "metadata": {},
   "source": [
    "## STEP 2: Import Libraries\n",
    "\n",
    "Import NumPy for numerical operations and Matplotlib for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment sanity check + imports\n",
    "import platform\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"OS:\", platform.system(), platform.release())\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(\"NumPy:\", np.__version__)\n",
    "    print(\"All libraries imported successfully!\")\n",
    "except ModuleNotFoundError as exc:\n",
    "    print(\"Missing dependency:\", exc)\n",
    "    print(\"Install with: python -m pip install numpy matplotlib\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q-learning-concept",
   "metadata": {},
   "source": [
    "## Q-Learning: Concept\n",
    "\n",
    "**Q-Learning** is a value-based reinforcement learning algorithm that learns the optimal action-value function Q(s, a).\n",
    "\n",
    "**Q-Table**: A table that stores Q-values for each state-action pair.\n",
    "- Q(s, a) = expected future reward when taking action 'a' in state 's'\n",
    "- Higher Q-value = better action\n",
    "\n",
    "**Q-Learning Update Rule**:\n",
    "```\n",
    "Q(s, a) = Q(s, a) + α * [R + γ * max(Q(s', a')) - Q(s, a)]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **α (alpha)**: Learning rate (how much to update)\n",
    "- **R**: Immediate reward\n",
    "- **γ (gamma)**: Discount factor (importance of future rewards)\n",
    "- **s'**: Next state\n",
    "- **max(Q(s', a'))**: Best Q-value in the next state\n",
    "\n",
    "**Exploration vs Exploitation**:\n",
    "- **Exploration**: Try random actions to discover better strategies\n",
    "- **Exploitation**: Use the best-known action\n",
    "- **ε-greedy**: Choose random action with probability ε, otherwise choose best action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3",
   "metadata": {},
   "source": [
    "## STEP 3: Create Grid World Environment\n",
    "\n",
    "We'll create a simple 4x4 grid world where:\n",
    "- The agent starts at position (0, 0)\n",
    "- The goal is at position (3, 3)\n",
    "- The agent can move: Up, Down, Left, Right\n",
    "- Reaching the goal gives +10 reward\n",
    "- Each step gives -1 reward (encourages finding shortest path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grid-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept: Define a simple grid world environment\n",
    "class GridWorld:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.start = (0, 0)\n",
    "        self.goal = (size-1, size-1)\n",
    "        self.state = self.start\n",
    "        \n",
    "        # Actions: 0=Up, 1=Down, 2=Left, 3=Right\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to start state\"\"\"\n",
    "        self.state = self.start\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take an action and return (next_state, reward, done)\"\"\"\n",
    "        row, col = self.state\n",
    "        \n",
    "        # Move based on action\n",
    "        if action == 0:  # Up\n",
    "            row = max(0, row - 1)\n",
    "        elif action == 1:  # Down\n",
    "            row = min(self.size - 1, row + 1)\n",
    "        elif action == 2:  # Left\n",
    "            col = max(0, col - 1)\n",
    "        elif action == 3:  # Right\n",
    "            col = min(self.size - 1, col + 1)\n",
    "        \n",
    "        self.state = (row, col)\n",
    "        \n",
    "        # Check if goal reached\n",
    "        if self.state == self.goal:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1  # Small penalty for each step\n",
    "            done = False\n",
    "        \n",
    "        return self.state, reward, done\n",
    "\n",
    "# Test the environment\n",
    "env = GridWorld()\n",
    "print(f\"Grid World Environment ({env.size}x{env.size})\")\n",
    "print(f\"Start: {env.start}\")\n",
    "print(f\"Goal: {env.goal}\")\n",
    "print(f\"Actions: Up(0), Down(1), Left(2), Right(3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4",
   "metadata": {},
   "source": [
    "## STEP 4: Implement Q-Learning Algorithm\n",
    "\n",
    "We'll implement the Q-Learning algorithm with:\n",
    "- Q-table initialization\n",
    "- ε-greedy action selection\n",
    "- Q-value updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept: Implement Q-Learning algorithm\n",
    "class QLearning:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate  # α\n",
    "        self.discount_factor = discount_factor  # γ\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        # Initialize Q-table: Q[state][action]\n",
    "        # State is represented as (row, col)\n",
    "        self.q_table = {}\n",
    "        for row in range(env.size):\n",
    "            for col in range(env.size):\n",
    "                self.q_table[(row, col)] = np.zeros(len(env.actions))\n",
    "    \n",
    "    def get_state_index(self, state):\n",
    "        \"\"\"Convert state tuple to index\"\"\"\n",
    "        return state\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Choose action using ε-greedy policy\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            # Exploration: random action\n",
    "            return np.random.choice(self.env.actions)\n",
    "        else:\n",
    "            # Exploitation: best action\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Update Q-value using Q-Learning update rule\"\"\"\n",
    "        current_q = self.q_table[state][action]\n",
    "        \n",
    "        if done:\n",
    "            target_q = reward\n",
    "        else:\n",
    "            target_q = reward + self.discount_factor * np.max(self.q_table[next_state])\n",
    "        \n",
    "        # Q-Learning update\n",
    "        self.q_table[state][action] = current_q + self.learning_rate * (target_q - current_q)\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay exploration rate\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Initialize Q-Learning agent\n",
    "q_agent = QLearning(env)\n",
    "print(\"Q-Learning agent initialized!\")\n",
    "print(f\"Initial Q-table size: {len(q_agent.q_table)} states\")\n",
    "print(f\"Q-values per state: {len(q_agent.q_table[(0,0)])} actions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5",
   "metadata": {},
   "source": [
    "## STEP 5: Train the Agent\n",
    "\n",
    "We'll train the agent for multiple episodes, allowing it to learn the optimal policy through exploration and exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept: Train the Q-Learning agent\n",
    "def train_agent(agent, env, episodes=1000):\n",
    "    \"\"\"Train the agent for specified number of episodes\"\"\"\n",
    "    episode_rewards = []\n",
    "    episode_steps = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Choose action\n",
    "            action = agent.choose_action(state)\n",
    "            \n",
    "            # Take action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Update Q-value\n",
    "            agent.update_q_value(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            # Prevent infinite loops\n",
    "            if steps > 100:\n",
    "                break\n",
    "        \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        episode_steps.append(steps)\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-100:])\n",
    "            avg_steps = np.mean(episode_steps[-100:])\n",
    "            print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}, Avg Steps = {avg_steps:.2f}, Epsilon = {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return episode_rewards, episode_steps\n",
    "\n",
    "# Train the agent\n",
    "print(\"Training Q-Learning agent...\")\n",
    "rewards, steps = train_agent(q_agent, env, episodes=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6",
   "metadata": {},
   "source": [
    "## STEP 6: Visualize Training Progress\n",
    "\n",
    "Let's plot how the agent's performance improves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept: Visualize training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot rewards over time\n",
    "ax1.plot(rewards, alpha=0.6, linewidth=0.5)\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Total Reward')\n",
    "ax1.set_title('Rewards per Episode')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot steps per episode\n",
    "ax2.plot(steps, alpha=0.6, linewidth=0.5, color='orange')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Steps per Episode')\n",
    "ax2.set_title('Steps per Episode')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal average reward (last 100 episodes): {np.mean(rewards[-100:]):.2f}\")\n",
    "print(f\"Final average steps (last 100 episodes): {np.mean(steps[-100:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step7",
   "metadata": {},
   "source": [
    "## STEP 7: Visualize Learned Policy\n",
    "\n",
    "Let's visualize the optimal policy the agent learned by showing the best action for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "policy-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept: Visualize the learned policy\n",
    "def visualize_policy(agent, env):\n",
    "    \"\"\"Visualize the learned policy as arrows on a grid\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Arrow directions for each action\n",
    "    arrows = {\n",
    "        0: (0, -0.3),  # Up\n",
    "        1: (0, 0.3),   # Down\n",
    "        2: (-0.3, 0),  # Left\n",
    "        3: (0.3, 0)   # Right\n",
    "    }\n",
    "    \n",
    "    arrow_labels = {0: '↑', 1: '↓', 2: '←', 3: '→'}\n",
    "    \n",
    "    # Draw grid\n",
    "    for row in range(env.size):\n",
    "        for col in range(env.size):\n",
    "            state = (row, col)\n",
    "            \n",
    "            # Color: start = green, goal = red, others = white\n",
    "            if state == env.start:\n",
    "                color = 'lightgreen'\n",
    "            elif state == env.goal:\n",
    "                color = 'lightcoral'\n",
    "            else:\n",
    "                color = 'white'\n",
    "            \n",
    "            # Draw cell\n",
    "            rect = plt.Rectangle((col - 0.5, env.size - row - 0.5), 1, 1, \n",
    "                                facecolor=color, edgecolor='black', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            # Draw arrow for best action\n",
    "            if state != env.goal:\n",
    "                best_action = np.argmax(agent.q_table[state])\n",
    "                dx, dy = arrows[best_action]\n",
    "                ax.arrow(col, env.size - row, dx, dy, \n",
    "                        head_width=0.15, head_length=0.1, fc='blue', ec='blue')\n",
    "                \n",
    "                # Show Q-value\n",
    "                q_val = agent.q_table[state][best_action]\n",
    "                ax.text(col, env.size - row - 0.3, f'{q_val:.1f}', \n",
    "                       ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    ax.set_xlim(-0.5, env.size - 0.5)\n",
    "    ax.set_ylim(-0.5, env.size - 0.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title('Learned Policy (Arrows show best action per state)')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.text(env.size + 0.5, env.size - 1, 'Start', color='green', fontsize=12, weight='bold')\n",
    "    ax.text(env.size + 0.5, env.size - 2, 'Goal', color='red', fontsize=12, weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the learned policy\n",
    "visualize_policy(q_agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-agent",
   "metadata": {},
   "source": [
    "## Test the Trained Agent\n",
    "\n",
    "Let's test the agent by having it navigate from start to goal using the learned policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concept: Test the trained agent\n",
    "def test_agent(agent, env, episodes=5):\n",
    "    \"\"\"Test the agent's learned policy\"\"\"\n",
    "    action_names = {0: 'Up', 1: 'Down', 2: 'Left', 3: 'Right'}\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        path = [state]\n",
    "        \n",
    "        print(f\"\\nEpisode {episode + 1}:\")\n",
    "        print(f\"  Start: {state}\")\n",
    "        \n",
    "        done = False\n",
    "        while not done and steps < 20:\n",
    "            # Use best action (no exploration)\n",
    "            action = np.argmax(agent.q_table[state])\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            print(f\"  Step {steps + 1}: {state} -> {action_names[action]} -> {next_state} (reward: {reward})\")\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            path.append(state)\n",
    "            \n",
    "            if done:\n",
    "                print(f\"  Goal reached! Total reward: {total_reward}, Steps: {steps}\")\n",
    "                break\n",
    "    \n",
    "    return path\n",
    "\n",
    "# Test the agent\n",
    "test_path = test_agent(q_agent, env, episodes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "key-learning",
   "metadata": {},
   "source": [
    "## Key Learning\n",
    "\n",
    "**Reinforcement Learning** is a powerful approach for learning through interaction:\n",
    "\n",
    "- **Agent learns by trial and error** — no labeled examples needed\n",
    "- **Q-Learning** learns optimal action-values through exploration and exploitation\n",
    "- **ε-greedy policy** balances exploration (trying new actions) and exploitation (using best-known actions)\n",
    "- **Rewards guide learning** — the agent learns to maximize cumulative reward\n",
    "\n",
    "**Key Concepts**:\n",
    "- **State**: Current situation\n",
    "- **Action**: What the agent can do\n",
    "- **Reward**: Feedback signal\n",
    "- **Q-Value**: Expected future reward for a state-action pair\n",
    "- **Policy**: Strategy for choosing actions\n",
    "\n",
    "**Real-World Applications**:\n",
    "- Game playing (Chess, Go, video games)\n",
    "- Robotics (navigation, manipulation)\n",
    "- Autonomous vehicles\n",
    "- Recommendation systems\n",
    "- Resource allocation\n",
    "\n",
    "**Next Steps**: Explore more advanced RL algorithms like Deep Q-Networks (DQN), Policy Gradient methods, and multi-agent reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
