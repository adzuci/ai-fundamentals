{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Class 8 Notebook – Bias and Ethics in AI/ML Basics\n",
        "\n",
        "This notebook introduces **bias and ethics** in AI and machine learning.\n",
        "\n",
        "As ML systems are deployed in hiring, lending, healthcare, and justice, understanding bias and fairness becomes essential:\n",
        "- **Data bias** – Training data reflects historical inequities\n",
        "- **Algorithmic bias** – Models can amplify or introduce new bias\n",
        "- **Fairness** – Different definitions (e.g., demographic parity, equalized odds) and trade-offs\n",
        "\n",
        "**Objective**: Set up the environment and explore core concepts for thinking critically about bias and responsible AI.\n",
        "\n",
        "**Key ideas**:\n",
        "- Bias can enter at data collection, labeling, feature selection, and model design\n",
        "- Fairness metrics can conflict; there is no single \"correct\" definition\n",
        "- Responsible AI includes transparency, interpretability, and ongoing monitoring\n",
        "\n",
        "Run the first code cell to confirm your environment works."
      ],
      "id": "intro"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run in the browser (no local setup)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adzuci/ai-fundamentals/blob/class-8-bias-and-ethics/class-8-bias-and-ethics/01_class_8_bias_and_ethics_basics.ipynb)\n",
        "\n",
        "> Tip: This notebook assumes you're comfortable with basic Python, pandas, and scikit-learn from earlier classes."
      ],
      "id": "colab-badge"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 1: Environment check and imports\n",
        "\n",
        "Verify that NumPy, pandas, and scikit-learn are available for building simple classifiers and analyzing predictions."
      ],
      "id": "step1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Concept: Environment sanity check for bias/ethics notebook\n",
        "import platform\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "print(f\"Python: {platform.python_version()}\")\n",
        "print(f\"OS: {platform.system()} {platform.release()}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"scikit-learn: {sklearn.__version__}\")\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "env-check"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is bias in ML?\n",
        "\n",
        "**Bias** in machine learning refers to systematic errors or unfairness in model behavior—often toward or against certain groups.\n",
        "\n",
        "- **Data bias**: Training data underrepresents groups, reflects historical discrimination, or has labeling errors that correlate with protected attributes.\n",
        "- **Algorithmic bias**: The model itself (e.g., regularization, threshold choices) produces different error rates or outcomes across groups.\n",
        "- **Feedback loops**: Deployed models influence future data (e.g., predictive policing), reinforcing existing bias.\n",
        "\n",
        "In later cells, we'll add examples and simple fairness checks."
      ],
      "id": "concepts"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 2: Create a toy dataset for bias exploration\n",
        "\n",
        "We create a simple dataset with **experience**, **test_score**, and **gender**—typical features in hiring or performance contexts. We'll use it to explore how models behave across groups and discuss fairness."
      ],
      "id": "0a4af082"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Concept: Create dataset for bias exploration\n",
        "# import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "# Create Dataset\n",
        "mydata = pd.DataFrame({\n",
        "    \"exp\": np.random.randint(0, 10, 100),\n",
        "    \"test_score\": np.random.randint(50, 100, 100),\n",
        "    \"gender\": np.random.choice([\"Male\", \"Female\"], 100)\n",
        "})"
      ],
      "id": "a1f833f2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}