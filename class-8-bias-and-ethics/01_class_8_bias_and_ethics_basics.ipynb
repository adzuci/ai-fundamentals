{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# Class 8 Notebook – Bias and Ethics in AI/ML Basics\n",
        "\n",
        "This notebook introduces **bias and ethics** in AI and machine learning.\n",
        "\n",
        "As ML systems are deployed in hiring, lending, healthcare, and justice, understanding bias and fairness becomes essential:\n",
        "- **Data bias** – Training data reflects historical inequities\n",
        "- **Algorithmic bias** – Models can amplify or introduce new bias\n",
        "- **Fairness** – Different definitions (e.g., demographic parity, equalized odds) and trade-offs\n",
        "\n",
        "**Objective**: Set up the environment and explore core concepts for thinking critically about bias and responsible AI.\n",
        "\n",
        "**Key ideas**:\n",
        "- Bias can enter at data collection, labeling, feature selection, and model design\n",
        "- Fairness metrics can conflict; there is no single \"correct\" definition\n",
        "- Responsible AI includes transparency, interpretability, and ongoing monitoring\n",
        "\n",
        "Run the first code cell to confirm your environment works."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "colab-badge",
      "metadata": {
        "id": "colab-badge"
      },
      "source": [
        "## Run in the browser (no local setup)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adzuci/ai-fundamentals/blob/class-8-bias-and-ethics/class-8-bias-and-ethics/01_class_8_bias_and_ethics_basics.ipynb)\n",
        "\n",
        "> Tip: This notebook assumes you're comfortable with basic Python, pandas, and scikit-learn from earlier classes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "step1",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## STEP 1: Environment check and imports\n",
        "\n",
        "Verify that NumPy, pandas, and scikit-learn are available for building simple classifiers and analyzing predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "env-check",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "env-check",
        "outputId": "183354c2-15ee-4433-9814-ffb9a3494651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.10.14\n",
            "OS: Darwin 25.2.0\n",
            "NumPy: 1.26.4\n",
            "pandas: 2.3.3\n",
            "scikit-learn: 1.7.2\n",
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Concept: Environment sanity check for bias/ethics notebook\n",
        "import platform\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "print(f\"Python: {platform.python_version()}\")\n",
        "print(f\"OS: {platform.system()} {platform.release()}\")\n",
        "print(f\"NumPy: {np.__version__}\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"scikit-learn: {sklearn.__version__}\")\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "concepts",
      "metadata": {
        "id": "concepts"
      },
      "source": [
        "## What is bias in ML?\n",
        "\n",
        "**Bias** in machine learning refers to systematic errors or unfairness in model behavior—often toward or against certain groups.\n",
        "\n",
        "- **Data bias**: Training data underrepresents groups, reflects historical discrimination, or has labeling errors that correlate with protected attributes.\n",
        "- **Algorithmic bias**: The model itself (e.g., regularization, threshold choices) produces different error rates or outcomes across groups.\n",
        "- **Feedback loops**: Deployed models influence future data (e.g., predictive policing), reinforcing existing bias.\n",
        "\n",
        "In later cells, we'll add examples and simple fairness checks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a4af082",
      "metadata": {
        "id": "0a4af082"
      },
      "source": [
        "## STEP 2: Create a toy dataset for bias exploration\n",
        "\n",
        "We create a simple dataset with **experience**, **test_score**, and **gender**—typical features in hiring or performance contexts. We'll use it to explore how models behave across groups and discuss fairness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a1f833f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "a1f833f2",
        "outputId": "fae28608-60b8-4085-aba1-02daf91c12e6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>exp</th>\n",
              "      <th>test_score</th>\n",
              "      <th>gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>61</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>83</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7</td>\n",
              "      <td>82</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>97</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>72</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>9</td>\n",
              "      <td>73</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>86</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>6</td>\n",
              "      <td>84</td>\n",
              "      <td>Male</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>7</td>\n",
              "      <td>93</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>4</td>\n",
              "      <td>89</td>\n",
              "      <td>Female</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   exp  test_score  gender\n",
              "0    6          61    Male\n",
              "1    3          83  Female\n",
              "2    7          82    Male\n",
              "3    4          97    Male\n",
              "4    6          72    Male\n",
              "5    9          73  Female\n",
              "6    2          86  Female\n",
              "7    6          84    Male\n",
              "8    7          93  Female\n",
              "9    4          89  Female"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Concept: Create dataset for bias exploration\n",
        "# import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "# Create Dataset\n",
        "mydata = pd.DataFrame({\n",
        "    \"exp\": np.random.randint(0, 10, 100),\n",
        "    \"test_score\": np.random.randint(50, 100, 100),\n",
        "    \"gender\": np.random.choice([\"Male\", \"Female\"], 100)\n",
        "})\n",
        "\n",
        "mydata.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e1c1cb",
      "metadata": {},
      "source": [
        "## STEP 3: Introduce bias into the dataset\n",
        "\n",
        "We create a **hired** column: first based on merit (exp > 5 and test_score > 70), then we **manually add bias** so males have a 70% chance of being marked hired regardless of qualifications. This simulates the kind of historical bias that can appear in real hiring data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b2f74b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concept: Introduce bias (males have higher chance of being hired)\n",
        "mydata[\"hired\"] = ((mydata[\"exp\"] > 5) & (mydata[\"test_score\"] > 70)).astype(int)\n",
        "\n",
        "# Add bias manually\n",
        "mydata.loc[mydata[\"gender\"] == \"Male\", \"hired\"] = np.where(\n",
        "    np.random.rand(len(mydata[mydata[\"gender\"] == \"Male\"])) > 0.3, 1, 0\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yIroZnQCpWge",
      "metadata": {
        "id": "yIroZnQCpWge"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
