{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3b69bc6",
      "metadata": {},
      "source": [
        "# Class 6 Notebook – Natural Language Processing (NLP) Basics\n",
        "\n",
        "This notebook introduces **Natural Language Processing (NLP)** using a small, end-to-end example:\n",
        "working with raw text, doing basic **tokenization** and **cleaning**, and preparing it for later modeling.\n",
        "\n",
        "We will connect code back to the Class 6 deck concepts:\n",
        "- Tokenization and stop words\n",
        "- Stemming vs lemmatization (conceptual)\n",
        "- TF–IDF (Term Frequency – Inverse Document Frequency) — **concept only here**\n",
        "- Basic text classification (e.g., positive vs negative phrases) — **covered in separate demos**\n",
        "\n",
        "**Objective**: Build a tiny NLP pipeline that:\n",
        "1. Pre-process text (tokenization, lowercasing, stopwords / simple cleaning)\n",
        "2. Create a tiny labeled text dataset\n",
        "3. Inspect how simple preprocessing changes the text\n",
        "\n",
        "A follow‑up notebook (`NLP_Demos.ipynb`) shows how to add TF–IDF and classifiers on top of this.\n",
        "\n",
        "Run the first code cell to confirm your environment works."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a69013",
      "metadata": {},
      "source": [
        "## Run in the browser (no local setup)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adzuci/ai-fundamentals/blob/class-6-natural-language-processing/class-6-natural-language-processing/01_class_6_nlp_basics.ipynb)\n",
        "\n",
        "> Tip: Make sure you are comfortable with basic Python and scikit-learn from Classes 2–3 before this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is NLP (Natural Language Processing)?\n",
        "\n",
        "**Natural Language Processing (NLP)** is about getting computers to work with **human language**:\n",
        "understanding text, extracting information, and generating language.\n",
        "\n",
        "Common applications (from the deck):\n",
        "- **Question Answering** – Answering questions from text or a knowledge base.\n",
        "- **Information Extraction** – Pulling structured fields from text (e.g., meeting *Time*, *Venue*).\n",
        "- **Machine Translation** – Translating between languages.\n",
        "- **Text summarization / keyword extraction** – Shortening long documents or extracting key phrases.\n",
        "- **Sentiment analysis** – Detecting whether text is positive, negative, or neutral.\n",
        "- **Context analysis / topic detection** – Understanding what a conversation or document is about.\n",
        "\n",
        "In this notebook we focus on a **very small slice** of NLP:\n",
        "- Turning text into numeric features (TF–IDF)\n",
        "- Training a small classifier for a toy sentiment-like task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526004b6",
      "metadata": {},
      "source": [
        "## STEP 1: Install and import libraries\n",
        "\n",
        "We use:\n",
        "- **NumPy** for arrays\n",
        "- **re** (regular expressions) for simple text cleaning.\n",
        "\n",
        "> Scikit-learn, TF–IDF, and classifiers are used later in `NLP_Demos.ipynb` and not in this intro notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b28a0a8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.10.14\n",
            "OS: Darwin 25.2.0\n",
            "NumPy: 2.2.6\n",
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Environment sanity check + imports\n",
        "import platform\n",
        "\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"OS:\", platform.system(), platform.release())\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    import re\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "    print(\"NumPy:\", np.__version__)\n",
        "    print(\"All libraries imported successfully!\")\n",
        "except ModuleNotFoundError as exc:\n",
        "    print(\"Missing dependency:\", exc)\n",
        "    print(\"Install with: python -m pip install numpy\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8e51e97f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP is amazing. It helps computers understand language\n",
            "Sentences: ['NLP is amazing.', 'It helps computers understand language']\n",
            "Words: ['NLP', 'is', 'amazing', '.', 'It', 'helps', 'computers', 'understand', 'language']\n"
          ]
        }
      ],
      "source": [
        "# Concept: Sentence and word tokenization with NLTK (for in-class exercise)\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Make sure the punkt models are available (needed once per environment).\n",
        "# If downloads fail (e.g., no internet), you can comment these out and still run the rest of the notebook.\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "text = 'NLP is amazing. It helps computers understand language'\n",
        "print(text)\n",
        "\n",
        "# Sentence-level tokenization\n",
        "my_sentences = sent_tokenize(text)\n",
        "print('Sentences:', my_sentences)\n",
        "\n",
        "# Word-level tokenization\n",
        "my_words = word_tokenize(text)\n",
        "print('Words:', my_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0ba002",
      "metadata": {},
      "source": [
        "## STEP 2: Create a tiny text dataset\n",
        "\n",
        "To keep things simple (and fast for teaching), we’ll create a **very small** dataset:\n",
        "short phrases labeled as **positive (1)** or **negative (0)**.\n",
        "\n",
        "In real projects you would load thousands of examples from files or a database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "522e3351",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POSITIVE | I love this product, it works great\n",
            "POSITIVE | This is the best course I have taken\n",
            "POSITIVE | Absolutely wonderful experience\n",
            "NEGATIVE | I hate this, it is terrible\n",
            "NEGATIVE | Really bad experience, would not recommend\n",
            "NEGATIVE | The support was awful and slow\n",
            "\n",
            "Number of examples: 6\n"
          ]
        }
      ],
      "source": [
        "# Concept: Tiny labeled text dataset (sentiment-like)\n",
        "texts = [\n",
        "    'I love this product, it works great',\n",
        "    'This is the best course I have taken',\n",
        "    'Absolutely wonderful experience',\n",
        "    'I hate this, it is terrible',\n",
        "    'Really bad experience, would not recommend',\n",
        "    'The support was awful and slow',\n",
        "]\n",
        "\n",
        "# Labels: 1 = positive, 0 = negative\n",
        "labels = np.array([1, 1, 1, 0, 0, 0])\n",
        "\n",
        "for text, label in zip(texts, labels):\n",
        "    sentiment = 'positive' if label == 1 else 'negative'\n",
        "    print(f'{sentiment.upper():8} | {text}')\n",
        "\n",
        "print('\\nNumber of examples:', len(texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1c6021",
      "metadata": {},
      "source": [
        "## STEP 3: Text pre-processing (concepts)\n",
        "\n",
        "Before vectorization, NLP systems usually do some **pre-processing**:\n",
        "- **Tokenization / Segmentation**: Split text into tokens (often words).\n",
        "- **Lowercasing**: Treat `Course` and `course` as the same token.\n",
        "- **Stop words**: Remove very common words (e.g., *the*, *and*, *from*) that carry little information.\n",
        "- **Stemming vs Lemmatization** (conceptual):\n",
        "  - *Stemming*: heuristic chop of suffixes (e.g., 'processing' → 'process') — may not be a valid word.\n",
        "  - *Lemmatization*: map a word to its dictionary form (lemma), e.g., 'doing' → 'do'.\n",
        "- **Named Entities**: Recognize real-world names (people, places, organizations).\n",
        "\n",
        "In this minimal example we’ll do only **lowercasing** and basic cleanup to keep the code small,\n",
        "but the ideas map directly onto more advanced pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4a8fa214",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL: I love this product, it works great\n",
            "CLEANED : i love this product it works great\n",
            "----------------------------------------\n",
            "ORIGINAL: This is the best course I have taken\n",
            "CLEANED : this is the best course i have taken\n",
            "----------------------------------------\n",
            "ORIGINAL: Absolutely wonderful experience\n",
            "CLEANED : absolutely wonderful experience\n",
            "----------------------------------------\n",
            "ORIGINAL: I hate this, it is terrible\n",
            "CLEANED : i hate this it is terrible\n",
            "----------------------------------------\n",
            "ORIGINAL: Really bad experience, would not recommend\n",
            "CLEANED : really bad experience would not recommend\n",
            "----------------------------------------\n",
            "ORIGINAL: The support was awful and slow\n",
            "CLEANED : the support was awful and slow\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Concept: Simple text cleaning function\n",
        "def simple_preprocess(text: str) -> str:\n",
        "    \"\"\"Lowercase and remove non-letter characters (very simple).\n",
        "    In a real system you would use a library (spaCy, NLTK, etc.).\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # Keep letters and spaces only\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Collapse multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "cleaned_texts = [simple_preprocess(t) for t in texts]\n",
        "for original, cleaned in zip(texts, cleaned_texts):\n",
        "    print(f'ORIGINAL: {original}')\n",
        "    print(f'CLEANED : {cleaned}')\n",
        "    print('-' * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af6a7e0",
      "metadata": {},
      "source": [
        "## STEP 4: TF–IDF vectorization\n",
        "\n",
        "Computers can’t work directly with raw strings, so we convert text to **vectors**.\n",
        "A very common approach is **TF–IDF (Term Frequency – Inverse Document Frequency)**:\n",
        "\n",
        "- **Term Frequency (TF)**: How often a term appears in a document.\n",
        "- **Inverse Document Frequency (IDF)**: How rare a term is across the corpus.\n",
        "- **TF–IDF score**: TF × IDF — high when a word is frequent in a document but not common everywhere.\n",
        "\n",
        "We’ll use scikit-learn’s `TfidfVectorizer` to:\n",
        "1. Tokenize the text\n",
        "2. Remove simple English stop words\n",
        "3. Build a vocabulary\n",
        "4. Compute TF–IDF features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d79b6899",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of TF–IDF matrix: (6, 18)\n",
            "Vocabulary size: 18\n",
            "Some features: ['absolutely' 'awful' 'bad' 'best' 'course' 'experience' 'great' 'hate'\n",
            " 'love' 'product']\n"
          ]
        }
      ],
      "source": [
        "# Concept: TF–IDF vectorization\n",
        "vectorizer = TfidfVectorizer(\n",
        "    preprocessor=simple_preprocess,\n",
        "    stop_words='english'  # drop common English stop words\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "print('Shape of TF–IDF matrix:', X.shape)\n",
        "print('Vocabulary size:', len(vectorizer.vocabulary_))\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print('Some features:', feature_names[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f8778fd",
      "metadata": {},
      "source": [
        "## STEP 5: Train a simple classifier\n",
        "\n",
        "We now have:\n",
        "- `X`: TF–IDF features (sparse matrix)\n",
        "- `labels`: 0/1 sentiment-like labels\n",
        "\n",
        "We’ll train a **Logistic Regression** classifier — a standard choice for text classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "2846aa8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n",
            "\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      1.00      0.67         1\n",
            "    positive       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/adam/.pyenv/versions/3.10.14/envs/ai-fundamentals/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/Users/adam/.pyenv/versions/3.10.14/envs/ai-fundamentals/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/Users/adam/.pyenv/versions/3.10.14/envs/ai-fundamentals/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Concept: Train/test split + classifier\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, labels, test_size=0.33, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
        "print('\\nClassification report:\\n')\n",
        "print(classification_report(y_test, y_pred, target_names=['negative', 'positive']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bbc3c9d",
      "metadata": {},
      "source": [
        "## STEP 6: Use the model on new text\n",
        "\n",
        "Now we can use the trained model to classify **new phrases** that were not seen during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "22f171a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POSITIVE | I really love this course\n",
            "NEGATIVE | The product was awful and I hate it\n",
            "NEGATIVE | It was okay, not great but not terrible either\n"
          ]
        }
      ],
      "source": [
        "# Concept: Predict sentiment of new text\n",
        "new_texts = [\n",
        "    'I really love this course',\n",
        "    'The product was awful and I hate it',\n",
        "    'It was okay, not great but not terrible either',\n",
        "]\n",
        "\n",
        "new_X = vectorizer.transform(new_texts)\n",
        "new_pred = clf.predict(new_X)\n",
        "\n",
        "for text, label in zip(new_texts, new_pred):\n",
        "    sentiment = 'positive' if label == 1 else 'negative'\n",
        "    print(f'{sentiment.upper():8} | {text}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5797a3bb",
      "metadata": {},
      "source": [
        "## Connecting back to the NLP deck\n",
        "\n",
        "In this tiny example we touched several key NLP ideas from the slides:\n",
        "\n",
        "- **Tokenization / Segmentation**: `TfidfVectorizer` tokenizes text into words under the hood.\n",
        "- **Stop words**: We removed common English stop words with `stop_words='english'`.\n",
        "- **TF–IDF**: We represented each document as a TF–IDF vector (term frequency × inverse document frequency).\n",
        "- **Classification**: We trained a Logistic Regression classifier on these vectors.\n",
        "- **Context**: Even this simple model uses some context (which words co-occur) but does *not* understand long-range context like a modern large language model (LLM).\n",
        "\n",
        "Concepts we *only mentioned* but did not code here:\n",
        "- **POS tagging** (Noun, Verb, etc.) and **Word Sense Disambiguation** (which meaning of a word like 'bank').\n",
        "- **Named Entities** (people, places, organizations).\n",
        "- **Topic models** (e.g., LDA, NMF) for discovering themes across many documents.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "- Swap `LogisticRegression` for another classifier (e.g., `LinearSVC`).\n",
        "- Experiment with your own small text dataset.\n",
        "- Explore topic modeling (NMF / LDA) on a collection of documents.\n",
        "- Compare this classical pipeline to modern transformer-based NLP (e.g., BERT, GPT)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82da0dd4",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fda12416",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
