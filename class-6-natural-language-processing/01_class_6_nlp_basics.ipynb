{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c3b69bc6",
      "metadata": {},
      "source": [
        "# Class 6 Notebook ‚Äì Natural Language Processing (NLP) Basics\n",
        "\n",
        "This notebook introduces **Natural Language Processing (NLP)** using a small, end-to-end example:\n",
        "working with raw text, doing basic **tokenization** and **cleaning**, and preparing it for later modeling.\n",
        "\n",
        "We will connect code back to the Class 6 deck concepts:\n",
        "- Tokenization and stop words\n",
        "- Stemming vs lemmatization (conceptual)\n",
        "- TF‚ÄìIDF (Term Frequency ‚Äì Inverse Document Frequency) ‚Äî **concept only here**\n",
        "- Basic text classification (e.g., positive vs negative phrases) ‚Äî **covered in separate demos**\n",
        "\n",
        "**Objective**: Build a tiny NLP pipeline that:\n",
        "1. Pre-process text (tokenization, lowercasing, stopwords / simple cleaning)\n",
        "2. Create a tiny labeled text dataset\n",
        "3. Inspect how simple preprocessing changes the text\n",
        "\n",
        "A follow‚Äëup notebook (`NLP_Demos.ipynb`) shows how to add TF‚ÄìIDF and classifiers on top of this.\n",
        "\n",
        "Run the first code cell to confirm your environment works."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a69013",
      "metadata": {},
      "source": [
        "## Run in the browser (no local setup)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adzuci/ai-fundamentals/blob/class-6-natural-language-processing/class-6-natural-language-processing/01_class_6_nlp_basics.ipynb)\n",
        "\n",
        "> Tip: Make sure you are comfortable with basic Python and scikit-learn from Classes 2‚Äì3 before this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is NLP (Natural Language Processing)?\n",
        "\n",
        "**Natural Language Processing (NLP)** is about getting computers to work with **human language**:\n",
        "understanding text, extracting information, and generating language.\n",
        "\n",
        "Common applications (from the deck):\n",
        "- **Question Answering** ‚Äì Answering questions from text or a knowledge base.\n",
        "- **Information Extraction** ‚Äì Pulling structured fields from text (e.g., meeting *Time*, *Venue*).\n",
        "- **Machine Translation** ‚Äì Translating between languages.\n",
        "- **Text summarization / keyword extraction** ‚Äì Shortening long documents or extracting key phrases.\n",
        "- **Sentiment analysis** ‚Äì Detecting whether text is positive, negative, or neutral.\n",
        "- **Context analysis / topic detection** ‚Äì Understanding what a conversation or document is about.\n",
        "\n",
        "In this notebook we focus on a **very small slice** of NLP:\n",
        "- Turning text into numeric features (TF‚ÄìIDF)\n",
        "- Training a small classifier for a toy sentiment-like task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526004b6",
      "metadata": {},
      "source": [
        "## STEP 1: Install and import libraries\n",
        "\n",
        "We use:\n",
        "- **NumPy** for arrays\n",
        "- **re** (regular expressions) for simple text cleaning.\n",
        "\n",
        "> Scikit-learn, TF‚ÄìIDF, and classifiers are used later in `NLP_Demos.ipynb` and not in this intro notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28a0a8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python: 3.10.14\n",
            "OS: Darwin 25.2.0\n",
            "NumPy: 2.2.6\n",
            "All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Environment sanity check + imports\n",
        "import platform  # Python / OS info only\n",
        "\n",
        "print(\"Python:\", platform.python_version())\n",
        "print(\"OS:\", platform.system(), platform.release())\n",
        "\n",
        "try:\n",
        "    import numpy as np  # numerical arrays and simple data work\n",
        "    import re  # regular expressions for basic text cleaning\n",
        "    import nltk  # core NLTK package (tokenization, corpora)\n",
        "    from nltk.corpus import stopwords  # common stop word lists (English, etc.)\n",
        "    from nltk.tokenize import word_tokenize, sent_tokenize  # word- and sentence-level tokenizers\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer  # TF‚ÄìIDF vectorizer from scikit-learn\n",
        "\n",
        "    print(\"NumPy:\", np.__version__)\n",
        "    print(\"All libraries imported successfully!\")\n",
        "except ModuleNotFoundError as exc:\n",
        "    print(\"Missing dependency:\", exc)\n",
        "    print(\"Install with: python -m pip install numpy nltk scikit-learn\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8e51e97f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLP is amazing. It helps computers understand language\n",
            "Sentences: ['NLP is amazing.', 'It helps computers understand language']\n",
            "Words: ['NLP', 'is', 'amazing', '.', 'It', 'helps', 'computers', 'understand', 'language']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /Users/adam/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Concept: Sentence and word tokenization with NLTK (for in-class exercise)\n",
        "# Download NLTK resources once per environment. If this is slow or you are offline,\n",
        "# you can comment these lines out and still run most of the notebook.\n",
        "nltk.download('punkt', quiet=True)      # sentence / word tokenizer models\n",
        "nltk.download('punkt_tab', quiet=True)  # extra punkt data in newer NLTK versions\n",
        "nltk.download('stopwords')              # stop word lists (used later or in demos)\n",
        "\n",
        "text = 'NLP is amazing. It helps computers understand language'\n",
        "print(text)\n",
        "\n",
        "# Sentence-level tokenization\n",
        "my_sentences = sent_tokenize(text)\n",
        "print('Sentences:', my_sentences)\n",
        "\n",
        "# Word-level tokenization\n",
        "my_words = word_tokenize(text)\n",
        "print('Words:', my_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af0ba002",
      "metadata": {},
      "source": [
        "## STEP 2: Create a tiny text dataset\n",
        "\n",
        "To keep things simple (and fast for teaching), we‚Äôll create a **very small** dataset:\n",
        "short phrases labeled as **positive (1)** or **negative (0)**.\n",
        "\n",
        "In real projects you would load thousands of examples from files or a database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8b3b6b41",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['NLP', 'is', 'an', 'interesting', 'field', 'of', 'AI', 'and', 'useful', 'to', 'create', 'a', 'Model']\n"
          ]
        }
      ],
      "source": [
        "# Concept: Tokenizing a different sentence\n",
        "mytext2 = \"NLP is an interesting field of AI and useful to create a Model\"\n",
        "\n",
        "my_words2 = word_tokenize(mytext2)\n",
        "print(my_words2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "522e3351",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POSITIVE | I love this product, it works great\n",
            "POSITIVE | This is the best course I have taken\n",
            "POSITIVE | Absolutely wonderful experience\n",
            "NEGATIVE | I hate this, it is terrible\n",
            "NEGATIVE | Really bad experience, would not recommend\n",
            "NEGATIVE | The support was awful and slow\n",
            "\n",
            "Number of examples: 6\n"
          ]
        }
      ],
      "source": [
        "# Concept: Tiny labeled text dataset (sentiment-like)\n",
        "texts = [\n",
        "    'I love this product, it works great',\n",
        "    'This is the best course I have taken',\n",
        "    'Absolutely wonderful experience',\n",
        "    'I hate this, it is terrible',\n",
        "    'Really bad experience, would not recommend',\n",
        "    'The support was awful and slow',\n",
        "]\n",
        "\n",
        "# Labels: 1 = positive, 0 = negative\n",
        "labels = np.array([1, 1, 1, 0, 0, 0])\n",
        "\n",
        "for text, label in zip(texts, labels):\n",
        "    sentiment = 'positive' if label == 1 else 'negative'\n",
        "    print(f'{sentiment.upper():8} | {text}')\n",
        "\n",
        "print('\\nNumber of examples:', len(texts))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa1c6021",
      "metadata": {},
      "source": [
        "## STEP 3: Text pre-processing (concepts)\n",
        "\n",
        "Before vectorization, NLP systems usually do some **pre-processing**:\n",
        "- **Tokenization / Segmentation**: Split text into tokens (often words).\n",
        "- **Lowercasing**: Treat `Course` and `course` as the same token.\n",
        "- **Stop words**: Remove very common words (e.g., *the*, *and*, *from*) that carry little information.\n",
        "- **Stemming vs Lemmatization** (conceptual):\n",
        "  - *Stemming*: heuristic chop of suffixes (e.g., 'processing' ‚Üí 'process') ‚Äî may not be a valid word.\n",
        "  - *Lemmatization*: map a word to its dictionary form (lemma), e.g., 'doing' ‚Üí 'do'.\n",
        "- **Named Entities**: Recognize real-world names (people, places, organizations).\n",
        "\n",
        "In this minimal example we‚Äôll do only **lowercasing** and basic cleanup to keep the code small,\n",
        "but the ideas map directly onto more advanced pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4a8fa214",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ORIGINAL: I love this product, it works great\n",
            "CLEANED : i love this product it works great\n",
            "----------------------------------------\n",
            "ORIGINAL: This is the best course I have taken\n",
            "CLEANED : this is the best course i have taken\n",
            "----------------------------------------\n",
            "ORIGINAL: Absolutely wonderful experience\n",
            "CLEANED : absolutely wonderful experience\n",
            "----------------------------------------\n",
            "ORIGINAL: I hate this, it is terrible\n",
            "CLEANED : i hate this it is terrible\n",
            "----------------------------------------\n",
            "ORIGINAL: Really bad experience, would not recommend\n",
            "CLEANED : really bad experience would not recommend\n",
            "----------------------------------------\n",
            "ORIGINAL: The support was awful and slow\n",
            "CLEANED : the support was awful and slow\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Concept: Simple text cleaning function\n",
        "def simple_preprocess(text: str) -> str:\n",
        "    \"\"\"Lowercase and remove non-letter characters (very simple).\n",
        "    In a real system you would use a library (spaCy, NLTK, etc.).\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # Keep letters and spaces only\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Collapse multiple spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "cleaned_texts = [simple_preprocess(t) for t in texts]\n",
        "for original, cleaned in zip(texts, cleaned_texts):\n",
        "    print(f'ORIGINAL: {original}')\n",
        "    print(f'CLEANED : {cleaned}')\n",
        "    print('-' * 40)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af6a7e0",
      "metadata": {},
      "source": [
        "## STEP 4: TF‚ÄìIDF vectorization (with scikit-learn)\n",
        "\n",
        "Computers can‚Äôt work directly with raw strings, so we convert text to **vectors**.\n",
        "A very common approach is **TF‚ÄìIDF (Term Frequency ‚Äì Inverse Document Frequency)**:\n",
        "\n",
        "- **Term Frequency (TF)**: How often a term appears in a document.\n",
        "- **Inverse Document Frequency (IDF)**: How rare a term is across the corpus.\n",
        "- **TF‚ÄìIDF score**: TF √ó IDF ‚Äî high when a word is frequent in a document but not common everywhere.\n",
        "\n",
        "In this step we use scikit-learn‚Äôs `TfidfVectorizer` (imported above) to:\n",
        "1. Tokenize the text\n",
        "2. Remove simple English stop words\n",
        "3. Build a vocabulary\n",
        "4. Compute TF‚ÄìIDF features for each example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d79b6899",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TfidfVectorizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Concept: TF‚ÄìIDF vectorization\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mTfidfVectorizer\u001b[49m(\n\u001b[1;32m      3\u001b[0m     preprocessor\u001b[38;5;241m=\u001b[39msimple_preprocess,\n\u001b[1;32m      4\u001b[0m     stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# drop common English stop words\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m X \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(texts)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShape of TF‚ÄìIDF matrix:\u001b[39m\u001b[38;5;124m'\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Concept: TF‚ÄìIDF vectorization (using scikit-learn)\n",
        "# This turns each cleaned text into a numeric vector that a classifier can use.\n",
        "vectorizer = TfidfVectorizer(\n",
        "    preprocessor=simple_preprocess,\n",
        "    stop_words='english'  # drop common English stop words\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "print('Shape of TF‚ÄìIDF matrix:', X.shape)\n",
        "print('Vocabulary size:', len(vectorizer.vocabulary_))\n",
        "\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print('Some features:', feature_names[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f8778fd",
      "metadata": {},
      "source": [
        "## STEP 5: Train a simple classifier (Logistic Regression)\n",
        "\n",
        "We now have:\n",
        "- `X`: TF‚ÄìIDF features (sparse matrix)\n",
        "- `labels`: 0/1 sentiment-like labels\n",
        "\n",
        "Here we use **Logistic Regression** from scikit-learn as a simple baseline text classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2846aa8d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.5\n",
            "\n",
            "Classification report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.50      1.00      0.67         1\n",
            "    positive       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.50         2\n",
            "   macro avg       0.25      0.50      0.33         2\n",
            "weighted avg       0.25      0.50      0.33         2\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/adam/.pyenv/versions/3.10.14/envs/ai-fundamentals/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/Users/adam/.pyenv/versions/3.10.14/envs/ai-fundamentals/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "/Users/adam/.pyenv/versions/3.10.14/envs/ai-fundamentals/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# Concept: Train/test split + classifier (Logistic Regression)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, labels, test_size=0.33, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=[\"negative\", \"positive\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bbc3c9d",
      "metadata": {},
      "source": [
        "## STEP 6: Use the model on new text\n",
        "\n",
        "Now we can use the trained model to classify **new phrases** that were not seen during training.\n",
        "We transform the new text with the **same** `TfidfVectorizer` and then call `clf.predict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22f171a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "POSITIVE | I really love this course\n",
            "NEGATIVE | The product was awful and I hate it\n",
            "NEGATIVE | It was okay, not great but not terrible either\n"
          ]
        }
      ],
      "source": [
        "# Concept: Predict sentiment of new text\n",
        "new_texts = [\n",
        "    \"I really love this course\",\n",
        "    \"The product was awful and I hate it\",\n",
        "    \"It was okay, not great but not terrible either\",\n",
        "]\n",
        "\n",
        "new_X = vectorizer.transform(new_texts)\n",
        "new_pred = clf.predict(new_X)\n",
        "\n",
        "for text, label in zip(new_texts, new_pred):\n",
        "    sentiment = \"positive\" if label == 1 else \"negative\"\n",
        "    print(f\"{sentiment.upper():8} | {text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5797a3bb",
      "metadata": {},
      "source": [
        "## Connecting back to the NLP deck\n",
        "\n",
        "In this tiny example we touched several key NLP ideas from the slides:\n",
        "\n",
        "- **Tokenization / Segmentation**: splitting text into sentences and words.\n",
        "- **Stop words & cleaning**: lowercasing and removing non-letter characters.\n",
        "- **TF‚ÄìIDF**: representing each document as a TF‚ÄìIDF vector.\n",
        "- **Classification**: training a simple Logistic Regression classifier on TF‚ÄìIDF features.\n",
        "\n",
        "Concepts we *only mentioned* but did not explore in depth here:\n",
        "- **POS tagging** (Noun, Verb, etc.) and **Word Sense Disambiguation** (which meaning of a word like 'bank').\n",
        "- **Named Entities** (people, places, organizations).\n",
        "- **Topic models** (e.g., LDA, NMF) for discovering themes across many documents.\n",
        "\n",
        "## Next steps\n",
        "\n",
        "- Try changing the tiny dataset and see how the classifier behaves.\n",
        "- Experiment with different preprocessing rules (e.g., keep punctuation, change stop words).\n",
        "- Swap `LogisticRegression` for another classifier (e.g., `LinearSVC`).\n",
        "- Explore topic modeling (NMF / LDA) or modern transformer-based NLP (e.g., BERT, GPT) in follow‚Äëup notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69362ff4",
      "metadata": {},
      "source": [
        "## NLTK vs scikit-learn (quick comparison)\n",
        "\n",
        "| Library       | Purpose                     | What it‚Äôs good at                               |\n",
        "|--------------|-----------------------------|-------------------------------------------------|\n",
        "| **NLTK**     | Natural Language Processing | Tokenization, stop words, stemming, lemmatization |\n",
        "| **scikit-learn** | Machine Learning           | TF‚ÄìIDF, feature extraction, classification models |\n",
        "\n",
        "**In practice:**\n",
        "- Use **NLTK** to prepare and clean text (tokens, stop words, stemming/lemmatization).\n",
        "- Use **scikit-learn** to turn text into **numbers** (e.g., TF‚ÄìIDF vectors) and train **models** (e.g., Logistic Regression)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9865bcea",
      "metadata": {},
      "source": [
        "## üå± Concept: Stemming\n",
        "\n",
        "**Stemming** reduces words to a root form by chopping off suffixes.\n",
        "\n",
        "Examples:\n",
        "- `running` ‚Üí `run`\n",
        "- `singing` ‚Üí `sing`\n",
        "- `studies` ‚Üí `studi`\n",
        "\n",
        "‚ö†Ô∏è **Stems are not always real words.** The goal is consistency, not perfect grammar.\n",
        "\n",
        "**Why we use stemming:**\n",
        "- Groups similar words together (e.g., `run`, `running`, `ran` ‚Üí `run`-like stem)\n",
        "- Reduces vocabulary size\n",
        "- Can improve model performance, especially on small datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a941c56",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concept: Stemming with NLTK's PorterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "words = [\"running\", \"singing\", \"talking\", \"playing\", \"run\", \"studies\", \"ran\"]\n",
        "\n",
        "mystemmer = PorterStemmer()\n",
        "\n",
        "mystems = [mystemmer.stem(w) for w in words]\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words :\", mystems)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82da0dd4",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
